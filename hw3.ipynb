{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import delle librerie**\n",
        "Qui importiamo NumPy per il caricamento dei .npy, Pathlib per gestire i percorsi, scikit-learn per l’MLP e la grid-search, e Matplotlib/Seaborn per la confusion matrix."
      ],
      "metadata": {
        "id": "tP3I2O25lBj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Librerie di base\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Pre-processing e modello\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Model selection\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Metriche e visualizzazione\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "mgsxwShAlF5T"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configurazione del percorso dati**\n",
        "DATA_DIR punta alla directory contenente i sei file x_*.npy e y_*.npy. L’assert ti segnala subito se qualcosa non è stato caricato."
      ],
      "metadata": {
        "id": "5DdM0c_ml454"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se i file .npy sono nella cartella 'dataset' del Colab:\n",
        "DATA_DIR = Path('https://drive.google.com/drive/folders/1ZGSD-7bI77imsKusnm5NzbVoChJsh9e8?usp=drive_link')\n",
        "\n",
        "# Opzionale: verifica che i file esistano\n",
        "for fname in [\"x_train.npy\",\"y_train.npy\",\"x_val.npy\",\"y_val.npy\",\"x_test.npy\",\"y_test.npy\"]:\n",
        "    assert (DATA_DIR/fname).exists(), f\"File mancante: {fname}\"\n",
        "print(\"Tutti i file .npy sono presenti in\", DATA_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "u51ShBcomBMm",
        "outputId": "81a42764-2273-438e-b671-24daf84a9b29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "File mancante: x_train.npy",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-3061333828.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Opzionale: verifica che i file esistano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"x_train.npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"y_train.npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"x_val.npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"y_val.npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"x_test.npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"y_test.npy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"File mancante: {fname}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tutti i file .npy sono presenti in\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: File mancante: x_train.npy"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Caricamento dei dati**\n",
        "Verifichiamo che le shape siano coerenti: ad es. X_train dovrebbe essere (N_train, 32, 32, 3) o simile."
      ],
      "metadata": {
        "id": "z1zCPaV9m3OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Carica i file .npy in array NumPy\n",
        "X_train = np.load(DATA_DIR/\"x_train.npy\")\n",
        "y_train = np.load(DATA_DIR/\"y_train.npy\")\n",
        "X_val   = np.load(DATA_DIR/\"x_val.npy\")\n",
        "y_val   = np.load(DATA_DIR/\"y_val.npy\")\n",
        "X_test  = np.load(DATA_DIR/\"x_test.npy\")\n",
        "y_test  = np.load(DATA_DIR/\"y_test.npy\")\n",
        "\n",
        "print(\"Shape delle matrici:\")\n",
        "print(\" X_train:\", X_train.shape, \" y_train:\", y_train.shape)\n",
        "print(\" X_val:  \",   X_val.shape,   \" y_val:  \",   y_val.shape)\n",
        "print(\" X_test: \",  X_test.shape,  \" y_test: \",  y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "LHMdYlUKm9_B",
        "outputId": "019238a1-646a-4f17-9096-2a3bcac791c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'https:/drive.google.com/drive/folders/1ZGSD-7bI77imsKusnm5NzbVoChJsh9e8?usp=drive_link/x_train.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-3164481407.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1. Carica i file .npy in array NumPy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"x_train.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"y_train.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_val\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"x_val.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_val\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"y_val.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https:/drive.google.com/drive/folders/1ZGSD-7bI77imsKusnm5NzbVoChJsh9e8?usp=drive_link/x_train.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Flatten e normalizzazione**\n",
        "Gli MLP di scikit-learn richiedono input 2D (n_samples, n_features): qui 32×32×3 = 3072 feature per immagine."
      ],
      "metadata": {
        "id": "SrOfF_vbnBpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_and_norm(X: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Converte X in float32, normalizza i pixel in [0,1]\n",
        "    e appiattisce in shape (N, 3072).\n",
        "    \"\"\"\n",
        "    X = X.astype(\"float32\") / 255.0\n",
        "    return X.reshape(X.shape[0], -1)\n",
        "\n",
        "X_train = flatten_and_norm(X_train)\n",
        "X_val   = flatten_and_norm(X_val)\n",
        "X_test  = flatten_and_norm(X_test)\n",
        "\n",
        "print(\"Dopo flatten:\", X_train.shape)"
      ],
      "metadata": {
        "id": "ehq7uOT4nD6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Standardizzazione**\n",
        "Portare media a 0 e varianza a 1 migliora la convergenza degli MLP."
      ],
      "metadata": {
        "id": "dG035imvnJ46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting dello scaler **solo** sul train, poi applicato a val/test\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val   = scaler.transform(X_val)\n",
        "X_test  = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "5KatfJmNnNvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Definizione del modello e griglia di iper-parametri**\n",
        "- max_iter: epoche massime; - early_stopping: interrompe se la validazione interna non migliora;\n",
        "- alpha: coopta il weight-decay (L2);\n",
        "- hidden_layer_sizes: numero di neuroni nei layer."
      ],
      "metadata": {
        "id": "WWkxDZRqnPdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Definisco un MLP \"di base\" con early stopping\n",
        "base_mlp = MLPClassifier(\n",
        "    max_iter=200,\n",
        "    early_stopping=True,\n",
        "    n_iter_no_change=15,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "# 2) Griglia di parametri da esplorare\n",
        "param_grid = {\n",
        "    \"hidden_layer_sizes\": [(256,), (512, 256)],\n",
        "    \"alpha\":                [1e-4, 1e-3],       # regolarizzazione L2\n",
        "    \"learning_rate_init\":   [1e-3, 5e-4],       # tasso di apprendimento iniziale\n",
        "}"
      ],
      "metadata": {
        "id": "gGIpNi2wnlOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Grid Search con cross-validation**\n",
        "GridSearchCV esplora tutte le combinazioni, esegue cross-validation, valuta con accuracy e infine ri‐addestra il modello migliore sull’intero X_train."
      ],
      "metadata": {
        "id": "cdYAmSojnqdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search = GridSearchCV(\n",
        "    estimator=base_mlp,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=3,            # 3‐fold CV sul training set\n",
        "    n_jobs=-1,       # usa tutti i core\n",
        "    verbose=2,       # output di avanzamento\n",
        "    refit=True       # ri-addestra il migliore su tutto il train\n",
        ")\n",
        "\n",
        "print(\"Avvio Grid Search...\")\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nMigliori iper-parametri:\")\n",
        "print(search.best_params_)\n",
        "print(f\"Accuracy media CV: {search.best_score_:.3f}\")"
      ],
      "metadata": {
        "id": "hwwY3gsUnuYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Valutazione sul validation set “esterno”**\n",
        "Una volta scelti i parametri, verifichiamo le prestazioni sullo split di validation che non è stato toccato dalla CV."
      ],
      "metadata": {
        "id": "szglBBhon4Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsione sulla validation “ufficiale”\n",
        "val_pred = search.predict(X_val)\n",
        "val_acc  = accuracy_score(y_val, val_pred)\n",
        "print(f\"Accuracy su validation esterno: {val_acc:.3f}\")"
      ],
      "metadata": {
        "id": "UvPZqJ2dn-a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Valutazione sul test set**\n",
        "Qui misuriamo accuracy, precision, recall e F1 su dati completamente “nuovi”."
      ],
      "metadata": {
        "id": "iu-C6meEn_ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = search.predict(X_test)\n",
        "test_acc  = accuracy_score(y_test, test_pred)\n",
        "print(f\"Accuracy su test set: {test_acc:.3f}\\n\")\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, test_pred, digits=3))"
      ],
      "metadata": {
        "id": "QAHkChf2pwjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Confusion Matrix**\n",
        "La confusion matrix aiuta a vedere quali classi vengono confuse più frequentemente."
      ],
      "metadata": {
        "id": "MfOyW3XDp651"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, test_pred)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix – Test Set\")\n",
        "plt.xlabel(\"Predetto\")\n",
        "plt.ylabel(\"Reale\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "skGz8zbop9n1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}